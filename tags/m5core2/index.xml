<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>M5core2 on Defios Lab.</title>
    <link>https://DefiosLab.github.io/tags/m5core2/</link>
    <description>Recent content in M5core2 on Defios Lab.</description>
    <generator>Hugo</generator>
    <language>jp</language>
    <lastBuildDate>Thu, 07 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://DefiosLab.github.io/tags/m5core2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>M5core2でリアルタイム７seg推論</title>
      <link>https://DefiosLab.github.io/post/m5core2_7seg/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://DefiosLab.github.io/post/m5core2_7seg/</guid>
      <description>&lt;h1 id=&#34;1-はじめに&#34;&gt;1. はじめに&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;本記事はM5core2内で7seg文字をニューラルネットワークでリアルタイム画像認識をさせようという記事です。&#xA;M5stackは約5cm×5cmの正方形のケースの中に、Wi-FiとBluetoothによる無線通信機能を搭載したESP32をはじめ、カラーディスプレイ、ボタン、スピーカー、IMU,MicroSDなどの周辺部品が一つのモジュールとしてまとまっているマイコンモジュールです。さらに、側面、背面のピンにケーブルを接続すれば色々なセンサー類が使用できるようになります。&#xA;M5core2はM5basicの上位機種で物理ボタンからタッチセンサに変更になったり、6軸IMUが追加されたり、バッテリー容量が3倍ほどになっていたりしています。&#xA;本記事を通じてM5core2でリアルタイムにAIを動かす際の参考になれば幸いです。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;2開発環境&#34;&gt;2.　開発環境&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;使用した機材、環境は以下になります。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;端末 : M5core2　(今回は試していないが、M5 basicなどの他のM5シリーズでも可能だと思われる)&lt;/li&gt;&#xA;&lt;li&gt;カメラ : UnitCam　(Wi-Fi通信 or uart通信で画像を送信できるカメラ)&lt;/li&gt;&#xA;&lt;li&gt;環境 : VSCode + PlatformIO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.m5stack.com/ja/core/core2&#34;&gt;&lt;img src =&#34;https://static-cdn.m5stack.com/resource/docs/products/core/core2/core2_01.webp&#34; width=&#34;20%&#34;&gt;&lt;/a&gt;&#xA;&lt;a href=&#34;https://www.switch-science.com/products/7231&#34;&gt;&lt;img src =&#34;https://www.switch-science.com/cdn/shop/products/f06a7f0c-15a6-4c4b-ae5d-94a41ff9df05_800x800.jpg?v=1699715018&#34; width=&#34;20%&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;!-- [![M5core2](https://static-cdn.m5stack.com/resource/docs/products/core/core2/core2_01.webp)](https://docs.m5stack.com/ja/core/core2) --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;今回使用したファイルは下記のリポジトリにまとめてあります。クローンして使用してください&#xA;&lt;a href=&#34;https://github.com/DefiosLab/M5_7seg_program&#34;&gt;https://github.com/DefiosLab/M5_7seg_program&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;3-モデル学習&#34;&gt;3. モデル学習&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;7seg画像認識のモデルを作成する際に下記サイトを参考に行い、データセットも下記サイトからお借りしました。とても分かりやすく画像認識のモデルを作成する際に必要な情報がまとめてあっておすすめです。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://child-programmer.com/ai/cnn-originaldataset-samplecode/#Google_Colaboratory_PythonKerasCNN&#34;&gt;【サンプルコード】Python・KerasでCNN機械学習。自作・自前画像のオリジナルデータセットで画像認識入門&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;データセットは下記サイトからダウンロードできます。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://child-programmer.com/download/seven-segment-digits-ocr-original-model-dataset/&#34;&gt;ダウンロード：7セグメントのデジタル数字画像認識用オリジナルデータセット&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;実際に学習に使用したプログラムは&amp;quot;learn_7seg.ipynb&amp;quot;になります&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;学習済みのmodel.h5ファイルが次のステップで必要になります&lt;/p&gt;&#xA;&lt;h1 id=&#34;4-モデルの量子化&#34;&gt;4. モデルの量子化&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;量子化とは重みなどのパラメータをより低bitで表すことでモデルの軽量化を行うモデル圧縮の1つの手法です。&lt;/li&gt;&#xA;&lt;li&gt;量子化の際に使用したプログラムは&amp;quot;quantize.ipynb&amp;quot;になります&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;1.下記のコードで整数量子化(入出力の型は元モデルのままfloat)を行います&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# settings&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;input_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;model.h5&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;keras_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_model(input_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;representative_dataset_gen&lt;/span&gt;():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cast(train_images[i], tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(input_image, [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; ([input_image])&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;converter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lite&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;TFLiteConverter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_keras_model(keras_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;converter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimizations &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lite&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Optimize&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DEFAULT]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;converter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;representative_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; representative_dataset_gen&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tflite_quant_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; converter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;convert()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;7seg_InOutFloat_intQuantize.tflite&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; o_:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    o_&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(tflite_quant_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;2.量子化を行ったmodel.tfliteをM5core2上で動くようにCファイルに変換します&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ! xxd -i &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;MODEL_TFLITE_PATH&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &amp;gt; model.c&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;xxd：ファイルを16進数表記で出力。「-i」オプションはC言語のインクルードファイル形式で表示する&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;5-tfmicroライブラリの作成&#34;&gt;5. tfmicroライブラリの作成&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PlatformIoでのM5core2の開発環境は以下のサイトを参考に行いました。&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://qiita.com/desertfox_i/items/a6ff7deaa0a0b3802bcd&#34;&gt;VSCodeとPlatformIOでM5Stack Core2開発&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Tensorflow liteをM5core2で使えるようにするためにtfliteのライブラリを自分で作成する必要があります&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;1.PlatformIOでtfmicroというプロジェクトを作成する(boardはM5stack Core2を選択)&lt;/li&gt;&#xA;&lt;li&gt;2.TensorFlow_esp32をcloneする&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    git clone git@github.com:DefiosLab/TensorFlow_esp32.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;3.クローンしてきたTensorflow_esp32/PlatformIO/tfmicro/lib/tfmicroをlib/に入れる&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cp -r Tensorflow_esp32/PlatformIO/tfmicro/lib/tfmicro &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;Project&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/lib&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;6-m5core2とカメラの接続&#34;&gt;6. M5core2とカメラの接続&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;M5core2とカメラはuartで通信を行います。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2.M5Core2とカメラをGroveケーブルで接続します&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
