<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Archives on Defios Lab.</title>
    <link>https://DefiosLab.github.io/post/</link>
    <description>Recent content in Archives on Defios Lab.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>jp</language>
    <lastBuildDate>Thu, 16 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://DefiosLab.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ゲームの世界にローカルLLMを！！LlamaCppUnityを公開しました</title>
      <link>https://DefiosLab.github.io/post/release_llamacppunity/</link>
      <pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/release_llamacppunity/</guid>
      <description>1．はじめに 近年、LLM（Large Language Models）の利活用が進んでおり、その応用範囲は広がっています。その中でゲーム開発において、LLMを活用したいと考える方も多いと思います。例えばリアルなNPCやプレイヤーの行動に応じた自由度の高い反応を実現したい等。
LLMをアプリに組み込む場合、ChatGPTを筆頭としたクラウドLLMと通信して行うのが主流になっていますが、生成量に応じた課金が必要でゲームへの導入には障壁があります。
そこで、デバイス内でLLMの推論処理を完結するRuntimeライブラリ、 LlamaCppUnityを公開しました。
2. LlamaCppUnity LlamaCppUnityはllama.cppをバックエンドとしたLLM Runtimeライブラリです。
llama.cppの機能をそのまま使えるので、量子化やGPGPU/SIMD実行, クロスプラットフォーム(Windows, Mac, Android)に対応しています。
MIT Licenseで公開しているので再配布・改造も自由です。(Contribution大歓迎です！)
↓は本ライブラリの紹介動画です。
こちらの動画内で使われているアプリは今回デモ用に作ったユニティちゃんと簡単な会話ができるアプリです。
以下に公開していますので興味がある方はぜひ触ってみてください！（Windows, Android対応）
モデルはELYZA-japanese-Llama-2-7bの2bit量子化モデルを使っています。
3. サンプルコード 推論を動かすには以下のコードで簡単に実行できます。Token毎に生成結果を返すStreamにも対応しています。
using LlamaCppUnity; public class LlamaSample : MonoBehaviour { void Start() { Llama test = new Llama(&amp;#34;&amp;lt;path/to/gguf&amp;gt;&amp;#34;); string result = test.Run(&amp;#34;Q: Name the planets in the solar system? A: &amp;#34;); //Output example: &amp;#34;1. Venus, 2. Mercury, 3. Mars,&amp;#34; //Stream Mode foreach (string text in test.RunStream(&amp;#34;Q: Name the planets in the solar system?</description>
    </item>
    
    <item>
      <title>M5core2でリアルタイム７seg推論</title>
      <link>https://DefiosLab.github.io/post/m5core2_7seg/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/m5core2_7seg/</guid>
      <description>1. はじめに 本記事はM5core2内で7seg文字をニューラルネットワークでリアルタイム画像認識をさせようという記事です。 M5stackは約5cm×5cmの正方形のケースの中に、Wi-FiとBluetoothによる無線通信機能を搭載したESP32をはじめ、カラーディスプレイ、ボタン、スピーカー、IMU,MicroSDなどの周辺部品が一つのモジュールとしてまとまっているマイコンモジュールです。さらに、側面、背面のピンにケーブルを接続すれば色々なセンサー類が使用できるようになります。 M5core2はM5basicの上位機種で物理ボタンからタッチセンサに変更になったり、6軸IMUが追加されたり、バッテリー容量が3倍ほどになっていたりしています。 本記事を通じてM5core2でリアルタイムにAIを動かす際の参考になれば幸いです。 2.　開発環境 使用した機材、環境は以下になります。 端末 : M5core2　(今回は試していないが、M5 basicなどの他のM5シリーズでも可能だと思われる) カメラ : UnitCam　(Wi-Fi通信 or uart通信で画像を送信できるカメラ) 環境 : VSCode + PlatformIO 今回使用したファイルは下記のリポジトリにまとめてあります。クローンして使用してください https://github.com/DefiosLab/M5_7seg_program 3. モデル学習 7seg画像認識のモデルを作成する際に下記サイトを参考に行い、データセットも下記サイトからお借りしました。とても分かりやすく画像認識のモデルを作成する際に必要な情報がまとめてあっておすすめです。
【サンプルコード】Python・KerasでCNN機械学習。自作・自前画像のオリジナルデータセットで画像認識入門
実際に学習に使用したプログラムは&amp;quot;learn_7seg.ipynb&amp;quot;になります
学習済みのmodel.h5ファイルが次のステップで必要になります
4. モデルの量子化 量子化とは重みなどのパラメータをより低bitで表すことでモデルの軽量化を行うモデル圧縮の1つの手法です。 量子化の際に使用したプログラムは&amp;quot;quantize.ipynb&amp;quot;になります 1.下記のコードでモデルの重みだけをfloat32からint8に変換するダイナミック量子化を行います output_model = &amp;#34;model.tflite&amp;#34; input_model = &amp;#39;model.h5&amp;#39; keras_model = tf.keras.models.load_model(input_model) converter = tf.lite.TFLiteConverter.from_keras_model(keras_model) tflite_quant_model = converter.convert() 2.量子化を行ったmodel.tfliteをM5core2上で動くようにCファイルに変換します ! xxd -i {MODEL_TFLITE_PATH} &amp;gt; model.c xxd：ファイルを16進数表記で出力。「-i」オプションはC言語のインクルードファイル形式で表示する 5. tfmicroライブラリの作成 PlatformIoでのM5core2の開発環境は以下のサイトを参考に行いました。
VSCodeとPlatformIOでM5Stack Core2開発
Tensorflow liteをM5core2で使えるようにするためにtfliteのライブラリを自分で作成する必要があります
1.PlatformIOでtfmicroというプロジェクトを作成する(boardはM5stack Core2を選択) 2.</description>
    </item>
    
    <item>
      <title>M5Stackで高速なセンシングを目指したら苦労した話</title>
      <link>https://DefiosLab.github.io/post/high_speed_sensing_m5/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/high_speed_sensing_m5/</guid>
      <description>この記事は「IoTLT Advent Calendar 2022」の23日目です。
1．M5Stackっていいよね M5Stackとは、ESP32を搭載したマイコンモジュールで、モノによりますが最初から液晶付いてるし、ボタン付いてるし、スピーカー/マイク、IMUなど初期搭載センサー山盛りで、Wi-FiやBluetoothまで繋がっちゃうというめっちゃ便利な開発機です。
便利さもさることながら、今まで私がIoT工作でよく使っていたRaspberryPiは最近個人では入手しづらいですが、M5Stackは安定して売っています！欲しいなって思ったときに秋葉原で買えます。
そんな入手し安さも相まって、最近は業務の製品開発でも使ったりお気に入りになりつつあるデバイスです。
pic.twitter.com/iZlu48cwvr
&amp;mdash; 大将(Taisyo) (@T_taisyou) December 21, 2022 2. M5Stackを本格的に使いたかった M5StackはIoT入門教材にも使われたり、個人的には初心者の勉強向きかなとも思っていたんですが「画面も付いてるし、SDスロット付いてるし、中身ESP32だし、これをベースに組み込みシステム作ってもいいんじゃない？」ということで今までPICとかAVRとかPSoCとかをベースにマイコンから基板起こしてLCDやSDカードドライバなどを自分で書いていたようなシステムのベースをこれに置き換えて、もっと開発を簡単にしよう！と本格的な使用を検討することになりました。
ということで、自作したコントローラーの入力でロボットを操作することを想定して、単純にA/Dで入力した値をそのままD/Aで出すデバイスを試しに作ってみようと思いました。
M5StackにはSDカードスロットや液晶が付いてるので、操作記録をSDに記録したり、今のロボットのパワー出力を画面にかっこよく表示させることも将来的にできるかなと思ってベースにM5を選んでみました。
仕様としては、コントローラ操作(アナログスティックとか)の細かい動きや素早い動きでも反応良くしたかったのと、 処理内容としては「ADCで値を取得して、ちょっと計算して、DACで出すだけ」というマイコンにやらせる仕事にしてはとても簡単だったので希望動作周期は1kHzとしました。
PICマイコンならもっとセンサーやタスクが多くてもこれぐらいの速度でADC-&amp;gt;DACできたし、ESP32がコアのM5Stackならバッチリっでしょ。と思っていました。
3. ADC/DACに何を使うか M5Stack Base V2.6の場合、GPIOで使えるADC, DACはそれぞれ2個ずつで、 扱える電圧は0~3.3V、分解能は12bitで「もうちょっと分解能欲しいときもあるかもだけど、とりあえずまぁいいかなぁ」と思っていたんですが、個体差あるかもしれませんがどうやらノイズが凄いらしいという情報もいくつかあったので、内蔵AD/DAは使わない方向で作ることにしました。
ESP32のADC、元々電圧範囲真ん中以外かなり線形性悪かったような気がします。M5Stack固有でさらにノイズ乗ってる可能性も高いです。
&amp;mdash; Kenta IDA (@ciniml) March 4, 2020 M5Stackといえばユニット(センサーなど)がケーブル1本接続するだけで簡単に使えるのが特徴の一つです。
探してみたらADC/DACモジュールもあったので 「これをM5に ちょいと くっつけて、 さっ とコード書いたらもう完成じゃん！！」 とM5の便利さに興奮しつつADC/DACはこれを使うことにしました。
ADCは0〜12V検出で最大分解能16bit、DACは最大分解能12bitで出力電圧0〜3.3Vです。
今回使おうと思っていたM5Stack Basicにはユニットを繋げられる穴が一つしかなかったのでハブも買いました。
将来的にいっぱい繋げることも考えて6ポートに増やせるやつを選びました。
チャンネルの取得制御の仕方はポーリング制御らしいです。
4-1. 速さが足りない - うっかりミス編 ちょっと察しのいい方はすでに先ほど貼った商品ページのスペックからオチが予想できていたかもしれませんが&amp;hellip;
速さが足りない！！
それも当たり前、M5StackのADCユニット(中身はADS1100)のサンプルレートはMAX 128Hzだからです。
仕様を知った時「遅い！！」と思わず最初は言ってしまったが、電子工作ではこれぐらいで十分のことも多くて、私の要求が異常なだけだなと反省。
ともかく、1kHzなんてハナから無理な構成でした。「仕様をちゃんと読んで買いましょう」という初歩的な教訓になりました。
4-2. 速さが足りない - 公式ユニットの限界編 ということでADCをどうしようか考えていたところ、M5Stack用 I/O拡張ユニット2なるものを発見。
なんとこれは1個で、ADCとしても使えるI/Oポートが8個も拡張できるらしい。
中身はSTM32F030マイコンで、データシートを読んでもADCは3.3V 12bit分解能で1MHzのスピードでできそうだったので変換スピードは問題なさそうでした。</description>
    </item>
    
    <item>
      <title>Universal IntrinsicsでSIMDプログラミング</title>
      <link>https://DefiosLab.github.io/post/cv-universal-intrinsic/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/cv-universal-intrinsic/</guid>
      <description>岩手県立大学アドベントカレンダー18日目の記事です（大遅刻）
自己紹介 はじめまして近藤鯛貴です。
岩手県立大学ソフトウェア情報学研究科の博士課程に在学しつつ、Defios株式会社というベンチャー企業を経営しています。
大学ではラズパイとかJetsonなどの小型コンピュータを対象に高速化の研究を行っており、会社ではAI/IoT分野を中心に受託開発やコンサルティング業務を行ってます。
最近Defiosで技術ブログを立ち上げました。せっかくなのでこちらから投稿します。
1．はじめに 現代の大体のCPUではSIMD（Single Instruction Multiple Data)命令が実行可能です。SIMDとは一つの命令で複数のデータを処理する並列化形態です。上手く使うことでソフトウェアを高速化することが出来ます。
しかし、プロセッサのアーキテクチャや製品種類によって対応しているSIMDのbit幅や命令セットが異なり、様々なコンピュータ上でSIMDを活用するアプリケーションを実装するのは少し厄介です。
ふと、画像処理ライブラリであるOpenCVのSIMD対応がどうなっているのか気になって調べたところ以下の記事を見つけました。
どうやら、OpenCVではUniversal Intrinsicというライブラリを用いてSIMDアーキテクチャの差異を吸収しているようです。
本記事ではこのUniversal Intrinsic(以下UI)の使い方と簡単な実装例を紹介します。
2．使い方 普通にOpenCVをインストールして、&amp;lt;opencv2/core/simd_intrinsics.hpp&amp;gt;をincludeすればUIを使うことが出来ます。
SIMDが有効になっているか、どのbit幅が使えるかはOpenCVのサンプルのsimd_basic.cppを実行することで確認できます。
以下AVX2環境でのsimd_basic.cppの実行結果
================== macro dump =================== CV_SIMD is defined: 1 CV_SIMD_WIDTH is defined: 32 CV_SIMD128 is defined: 1 CV_SIMD256 is defined: 1 CV_SIMD512 is defined: 0 CV_SIMD_64F is defined: 1 CV_SIMD_FP16 is defined: 0 ================= sizeof checks ================= sizeof(v_uint8) = 32 sizeof(v_int32) = 32 sizeof(v_float32) = 32 ================== arithm check ================= (vx_setall_u8(10) + vx_setall_u8(45)).</description>
    </item>
    
    <item>
      <title>M5Stackで始めるIoT開発入門〜学習リモコン〜</title>
      <link>https://DefiosLab.github.io/post/remote_contoroller_mqtt/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/remote_contoroller_mqtt/</guid>
      <description>1．はじめに 本記事は、IoT開発初心者である私がM5Stackで学習リモコンを作成し、ネットワーク経由で家電を操作してみようという内容です。
M5StackはWi-FiとBluetoothによる無線通信機能を備えたESP32や液晶ディスプレイ、microSDカードスロットなどの周辺機器がまとまった小型のマイコンモジュールです。ユニット(センサーなど)を接続することで簡単にセンサーから値を取得できます。この記事を通して、M5Stackでの開発やMQTTなど基本的なIoT開発の仕方を勉強するきっかけや参考になればいいと思います。
2．開発環境と構成図 今回作成するシステムの構成図はこんな感じです。ただ、学習リモコンを作るだけではIoTにはならないので、ネットワーク経由でも操作できるようにしました。IoTでの通信といえばMQTTと聞いたので勉強も兼ねて実装していきたいと思います。
開発環境は以下の通りです。M5GO IoTスターターキットを購入すれば、IRセンサーの他にENVⅢ(温湿度・気圧)センサーなども付いてくるので初めてIoT開発する方にはおすすめです。前提として、Arduino IDEでのM5Stackの環境は整っていることとします。
端末：M5GO (M5STACK BASICでもOK) センサー：IRセンサー(赤外線センサー) サーバー：Raspberry Pi 4 (Linuxの入ったPCなら代替可) 3．学習リモコン作成 3.1．準備 最初に学習リモコンから作成していきたいと思います。まずは、IRremoteESP8266のライブラリをインストールします。
3.2．受信編 次に既存のリモコンから赤外線データを取得します。今回私は、電気をON・OFFするこちらのアイリスオオヤマ製のリモコンを使用します。
まずは、IRremoteESP8266のサンプルコードを取得します。 赤外線を受信するサンプルコードは、「ファイル」→「スケッチ例」→「IRremoteESP8266」の「IRrecvDumpV2」から取得できます。
M5GOでは、受信するPIN番号が36、送信するPIN番号が26番です。なので、受信するPIN番号を36に変更する必要があります。
const uint16_t kRecvPin = 36; 変更したら、コンパイル＋書き込みを行い、シリアルモニターを開いてIRセンサーに向けてリモコンのボタンを押しデータ受信させます。今回は「ON・OFF」、「明るく」、「暗い」のデータを受信します。赤外線データを受信すると以下のようなにシリアルモニタに表示されるのでuint16_t raw_data[85]のメモを取ります。 3.3．送信編 赤外線データを受信したら、次に学習リモコンを作成します。現時点では、Web UIやMQTTの実装はしていないので、ボタンが押された場合データを送信するように実装します。 Aボタンが「切/入」、Bボタンが「明るく」、Cボタンが「暗く」のデータを送信するように実装していきます。 コードについては、コード内にコメントを書いたので説明は省略します。コンパイル＋書き込みを行い、実際に家電(今回は電気)を操作できたら学習リモコンは完成です！
#include &amp;lt;M5Stack.h&amp;gt; #include &amp;lt;IRremoteESP8266.h&amp;gt; #include &amp;lt;IRsend.h&amp;gt; #define DATA_SIZE 85 // 送信する赤外線データのサイズ #define TRANSMIT_CAPTURE_SIZE 38 // 周波数. 赤外線リモコンの仕様が38khzなので、38で固定 int ir_send_pin = 26; // 送信するPIN番号 IRsend irsend(ir_send_pin); // 送信する赤外線データ uint16_t on_off[85] = {2022, 1004, 5584,... , 1494, 532, 512}; // 切/入 uint16_t brightly[85] = {2050, 978, 5586,.</description>
    </item>
    
    <item>
      <title>このブログについて</title>
      <link>https://DefiosLab.github.io/post/about/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://DefiosLab.github.io/post/about/</guid>
      <description>Defios株式会社 DefiosLab所長のTaisyoです。
普段AIやIoT関係の開発を行う弊社の社員が、技術から遊び、岩手での仕事の様子などあらゆるテーマについて自由に書く何でもブログを開設しました。
これから不定期に記事を追加していく予定です。
記事についてのコメントは#Defios_Labのハッシュタグで、TwitterやFacebookに投稿してもらえると嬉しいです！
ブログのデザインはレトロなPCやゲームが好きな私のこだわりのデザインですが、ちょっと見づらいかも知れませんね&amp;hellip;
・・・そのようなWebデザインに関する意見もお待ちしてます！
This blog is written by an employee of Defios Corporation, an IT company that develops AI and IoT.
I write articles mainly about technology.
I also write articles about Iwate Prefecture in Japan, where the company&amp;rsquo;s office is located.
― Director of DefiosLaboratory Takeda Hiromasa</description>
    </item>
    
  </channel>
</rss>
